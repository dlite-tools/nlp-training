{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66xEVnCfSaqh"
      },
      "source": [
        "# Workshop WDL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTsiNumpQK09"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/dlite-tools/nlp-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dnXqDk7YSHN"
      },
      "outputs": [],
      "source": [
        "%cd nlp-training/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPndOBabSIui"
      },
      "outputs": [],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGZgf1jGHaMy"
      },
      "outputs": [],
      "source": [
        "!pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILGjU0llGuTH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tempfile\n",
        "\n",
        "import torch\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import MLFlowLogger\n",
        "from pytorch_lightning.callbacks import (\n",
        "    EarlyStopping,\n",
        "    ModelCheckpoint,\n",
        "    LearningRateMonitor\n",
        ")\n",
        "\n",
        "from inference.architectures.text_classification import BaselineModel\n",
        "from inference.data_processors.transformers import BaseTransformer\n",
        "from inference.data_processors.processor import Processor\n",
        "from inference.data_processors.transformers.preprocessing import VocabTransform\n",
        "from training.trainer import TextClassificationTrainer\n",
        "from training.datasets.text_classification import AGNewsDataModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZsZF9scGzB6"
      },
      "source": [
        "## Hyper-parameters and other Settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aORECtFG5IT"
      },
      "outputs": [],
      "source": [
        "NUMBER_CLASSES = 4\n",
        "N_EPOCHS = 10\n",
        "EMBED_DIM = 64\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 8\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(monitor=\"valid_loss\", mode=\"min\", save_weights_only=True)\n",
        "early_stop_callback = EarlyStopping(monitor=\"valid_loss\", mode=\"min\", patience=4)\n",
        "learning_rate_monitor = LearningRateMonitor()\n",
        "\n",
        "mf_logger = MLFlowLogger(\n",
        "    experiment_name=\"AG News - Text Classification\",\n",
        "    run_name=\"Baseline\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmYiiI0AG8Bq"
      },
      "source": [
        "## Data transformation pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a Tokenizer\n",
        "\n",
        "Implement a class that inherits from `BaseTransformer`.\n",
        "\n",
        "This class must have implemented the method `__call__` that receives\n",
        "a string and return a list of strings."
      ],
      "metadata": {
        "id": "GOd_DJznBnmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "class Tokenize(BaseTransformer):\n",
        "    def __call__(self, text: str) -> List[str]:\n",
        "        # Receive a string and split it into a list of strings\n",
        "        pass"
      ],
      "metadata": {
        "id": "uA_59Xes-2UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processor\n",
        "\n",
        "The processor is a object that will sequentially apply the transformation to the data."
      ],
      "metadata": {
        "id": "CFJZbsG_Cd45"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdFeoqzaG_Bd"
      },
      "outputs": [],
      "source": [
        "vocab = VocabTransform()\n",
        "preprocessing = [\n",
        "    Tokenize(),\n",
        "    vocab\n",
        "]\n",
        "processor = Processor(preprocessing=preprocessing)\n",
        "\n",
        "\n",
        "vocab.build_vocab(processor, AG_NEWS(split='train'))\n",
        "\n",
        "print(f\"\\nVocabulary as size of {len(vocab)}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to modify the Tokenize to also remove symbols\n",
        "\n",
        "(tip: use `from string import punctuation` to get the punctuation that should be removed)\n"
      ],
      "metadata": {
        "id": "M86GG9O1GOFO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o68hwRXHCz6"
      },
      "source": [
        "## Setup data module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlK43bsuHGoq"
      },
      "outputs": [],
      "source": [
        "data_module = AGNewsDataModule(processor=processor, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPAQKRanHIK7"
      },
      "source": [
        "## Model and Model Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v44b1cYVGBFx"
      },
      "outputs": [],
      "source": [
        "model = BaselineModel(vocab_size=len(vocab), embed_dim=EMBED_DIM, num_class=NUMBER_CLASSES)\n",
        "\n",
        "model_trainer = TextClassificationTrainer(\n",
        "    model=model,\n",
        "    num_class=NUMBER_CLASSES\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    callbacks=[model_checkpoint, early_stop_callback],\n",
        "    max_epochs=N_EPOCHS,\n",
        "    logger=mf_logger,\n",
        "    gpus=torch.cuda.device_count(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PaKrkEFHQ90"
      },
      "source": [
        "## Training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmDlcqrUHUSi"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model_trainer, data_module)\n",
        "trainer.test(datamodule=data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets try to add Data augmentation to our training\n",
        "\n",
        "For that you can create a new transform based on `Tokenize`.\n",
        "Since we only want to apply on training set, we should set attribute `_data_aug` to `True`\n",
        "\n",
        "Tips:\n",
        "\n",
        "```\n",
        "class SentenceAugmentation(BaseTransformer):\n",
        "    _data_aug = True\n",
        "\n",
        "    def __call__(self, text: str) -> str:\n",
        "        pass\n",
        "```\n",
        "\n",
        "```\n",
        ">>>import nltk\n",
        ">>>import nlpaug.augmenter.word as naw\n",
        ">>>nltk.download('wordnet')\n",
        ">>>nltk.download('omw-1.4')\n",
        ">>>nltk.download('averaged_perceptron_tagger')\n",
        ">>>aug = naw.SynonymAug()\n",
        ">>>aug.augment('This is our random workshop on World Data League.')\n",
        "'This is our random workshop on Earthly concern Datum League.'\n",
        "```"
      ],
      "metadata": {
        "id": "QnCNwh34KZ4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using NLPiper Integration"
      ],
      "metadata": {
        "id": "pBq5XT9qGw-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpiper\n",
        "from inference.data_processors.transformers.preprocessing import NLPiperIntegration\n",
        "\n",
        "vocab = VocabTransform()\n",
        "preprocessing = [\n",
        "    NLPiperIntegration(pipeline=nlpiper.core.Compose([\n",
        "        nlpiper.transformers.cleaners.CleanPunctuation(),\n",
        "        nlpiper.transformers.tokenizers.BasicTokenizer(),\n",
        "        nlpiper.transformers.normalizers.CaseTokens(),\n",
        "    ])),\n",
        "    vocab\n",
        "]\n",
        "\n",
        "processor = Processor(preprocessing=preprocessing)\n",
        "vocab.build_vocab(processor, AG_NEWS(split='train'))\n",
        "\n",
        "print(f\"Vocabulary as size of {len(vocab)}.\")"
      ],
      "metadata": {
        "id": "TO-_44XfGwqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mf_logger = MLFlowLogger(\n",
        "    experiment_name=\"AG News - Text Classification\",\n",
        "    run_name=\"Baseline w/NLPiper\",\n",
        ")\n",
        "\n",
        "model = BaselineModel(vocab_size=len(vocab), embed_dim=EMBED_DIM, num_class=NUMBER_CLASSES)\n",
        "\n",
        "model_trainer = TextClassificationTrainer(\n",
        "    model=model,\n",
        "    num_class=NUMBER_CLASSES\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    callbacks=[model_checkpoint, early_stop_callback, learning_rate_monitor],\n",
        "    max_epochs=N_EPOCHS,\n",
        "    logger=mf_logger,\n",
        "    gpus=torch.cuda.device_count(),\n",
        ")"
      ],
      "metadata": {
        "id": "R4XJ5V0vJz6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model_trainer, data_module)\n",
        "trainer.test(datamodule=data_module)"
      ],
      "metadata": {
        "id": "frZ93hEqKRZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zip MLFlow logs"
      ],
      "metadata": {
        "id": "pfy8SgUeFCY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !zip -r mlruns.zip mlruns/"
      ],
      "metadata": {
        "id": "K6ab4Fpy-5Z7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Workshop_WDL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}